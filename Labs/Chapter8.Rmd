---
title: "Chapter 8 Lab"
html_document:
  toc: true
  toc_depth: 2
  toc_float:
    collapsed: false
    smooth_scroll: false
output: html_notebook
---

# Setup
We work on the `Carseats` data, and we are all about growing a tree from carseats!

```{r}
rm(list = ls())
search()
library(ISLR)
library(tree)
# knowing the data and variables
dim(Carseats)
summary(Carseats)
sum(is.na(Carseats))  # no missing
# learning about response
summary(Carseats$Sales)  # unit unknown, mean and median 7.5
hist(Carseats$Sales)  # not much skewed
# attaching the data: not recommended, but we follow the ISLR book
attach(Carseats)
```

After the initial setup, we create a dummy variable from `Sales` to work on as the response.
```{r}
High = ifelse(Sales > 8, "YES", "No")
summary(High)
```

The variable `High` is a vector of strings. It resides in the Global Environment and that is why `summary(High)` gives us no information about the values it takes. So the next step is to add the variable to the working data:
```{r}
Carseats = data.frame(Carseats, High)
```

Time to grow a tree:
```{r}
tree.carseats = tree(High ~ . - Sales, data = Carseats)
summary(tree.carseats)
```

We see residual mean deviance in the output. How is the residual mean deviance computed? It is equal to two times the ratio of log-likelihoods of the saturated model to the model being considered:

$$
Dev = 2\times \frac{\mathcal{L} ( y|\theta_s)}{\mathcal{L}(y|\theta_0)}
= 2\times \frac {1} { \log\Big(\prod_{m,k} \big({\hat{p}_{mk}}^{n_{mk}}\big)\Big) }
= -2\sum_m \sum_k n_{mk}\log \hat{p}_{mk}\,,
$$

where $\theta_s$ is the vector of parameters for the saturated model, and $\theta_0$ is the one for the model we consider (which is of course nested in the saturated model). The probability distributions are assumed to be multimonial. The saturated model is tree with a leaf for every observation. Hence, the saturated tree perfectly fits the data and its estimates $\hat{p}_{mk}$ are all equal to 1. The mean deviance is computed by dividing deviance by $n - |T|$.  
Next, we graphically represent the tree:
```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0, cex = 0.6)
```
The ability to visualize a tree is one of the most attractive properties of trees. Shelving location appears to be the most important indicator of Sales. The option `pretty = 0` makes the graph display category names, rather than single letters for each category. More details on the tree can be seen by typing the tree's name:

```{r}
summary(Carseats$High)
tree.carseats
```
Each node is represented with a number. One can trace back a node by dividing this number by two and taking the integer part of it. The resulting number denotes the parent node. By sequentially doing this, we can trace the node back to the root. * denotes the terminal nodes.  
The other information are the number of observations in each branch, the deviance, the overall prediction for the branch, and the fraction of leaves in that branch that take on values "NO" and "YES", respectively.  
Why is "NO" presented before "YES"? It is probably due to the way the variable is defined. However, which value is the baseline does not make much of a difference, here (although it makes a difference in other cases, e.g. when we want to interpret linear regression results).

What about the test error rate?
```{r}
set.seed(2)
train = sample(nrow(Carseats), 200)
test = -train
test.carseats = Carseats[test, ]
test.high = High[test] 
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
tree.preds = predict(tree.carseats, newdata = test.carseats, type = "class")
head(tree.preds)
table(tree.preds, test.high)
(86+57)/200
```
The test error rate estimate is 72%, while the training error rate was estimated to be 91%. 

Pruning the tree is done through cross-validation over a sequence of trees found by cost-complexity pruning.
```{r}
set.seed(3)
cv.tree(tree.carseats, FUN = prune.misclass)
```

The `FUN` argument in `cv.tree` determines the nested sequence of subtrees and is the output of the command `prune.tree()`, which has uses either of the following estimates for error: 

1. Deviance, which is the default
2. Misclassification error, which is done through equating the argument `FUN` to either `prune.tree(method = "misclass")` or its short form `prune.misclass`

In the output of `cv.tree` above, `$size` is the number of leaves, and `$k` corresponds to $\alpha$. The output `$dev` corresponds to the error, which is misclassification error in this case, despite its name. The value of $\alpha = -\infty$ would be the largest possible tree with RSS = 0, which has 27 leaves here.

## Summary

* We can use `ifelse` to create new factor variables.
    + Remember to merge it with the data set
* After fitting the tree, we learned how to
    + plot it
    + see its nodes and details 
    + compute test error rate
        - In `predict.tree()`, the argument `type = class` used to get factor predictions
    + prune it: use `cv.tree` with the argument `FUN = prune.misclass`
* Remember to set the seed before doing validation set approach or CV





